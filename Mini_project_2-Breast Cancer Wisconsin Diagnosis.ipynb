{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa636f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f3d380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Dell/Downloads/data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4867752f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6d5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 32', 'id'], axis = 1)\n",
    "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c228f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.diagnosis.values\n",
    "x_data = data.drop(['diagnosis'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70142d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n",
      "C:\\Users\\Dell\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "x = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60b38e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (32, 483)\n",
      "x test:  (32, 86)\n",
      "y train:  (483,)\n",
      "y test:  (86,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "\tx, y, test_size = 0.15, random_state = 42)\n",
    "\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "print(\"x train: \", x_train.shape)\n",
    "print(\"x test: \", x_test.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print(\"y test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa768d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_and_bias(dimension):\n",
    " w = np.full((dimension, 1), 0.01)\n",
    " b = 0.0\n",
    " return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e35a61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = np.dot(w.T, x_train)+b\n",
    "def sigmoid(z):\n",
    " y_head = 1/(1 + np.exp(-z))\n",
    " return y_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5c22b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w, b, x_train, y_train):\n",
    " z = np.dot(w.T, x_train) + b\n",
    " y_head = sigmoid(z)\n",
    " loss = - y_train * np.log(y_head) - (1 - y_train) * np.log(1 - y_head)\n",
    "\n",
    " cost = (np.sum(loss)) / x_train.shape[1]\t \n",
    "\n",
    "\n",
    " derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) / x_train.shape[1] \n",
    " derivative_bias = np.sum(y_head-y_train) / x_train.shape[1]\t\t\t\t \n",
    " gradients = {\"derivative_weig54ht\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    " return cost, gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83c1c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w, b, x_train, y_train, learning_rate, number_of_iterarion):\n",
    "\tcost_list = []\n",
    "\tcost_list2 = []\n",
    "\tindex = []\n",
    "\n",
    "\t# updating(learning) parameters is number_of_iterarion times\n",
    "\tfor i in range(number_of_iterarion):\n",
    "\t\t# make forward and backward propagation and find cost and gradients\n",
    "\t\tcost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n",
    "\t\tcost_list.append(cost)\n",
    "\n",
    "\t\t# lets update\n",
    "\t\tw = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "\t\tb = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "\t\tif i % 10 == 0:\n",
    "\t\t\tcost_list2.append(cost)\n",
    "\t\t\tindex.append(i)\n",
    "\t\t\tprint (\"Cost after iteration % i: % f\" %(i, cost))\n",
    "\n",
    "\t# update(learn) parameters weights and bias\n",
    "\tparameters = {\"weight\": w, \"bias\": b}\n",
    "\tplt.plot(index, cost_list2)\n",
    "\tplt.xticks(index, rotation ='vertical')\n",
    "\tplt.xlabel(\"Number of Iterarion\")\n",
    "\tplt.ylabel(\"Cost\")\n",
    "\tplt.show()\n",
    "\treturn parameters, gradients, cost_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8f770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553bef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f87386a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, x_test):\n",
    "\t# x_test is a input for forward propagation\n",
    "\tz = sigmoid(np.dot(w.T, x_test)+b)\n",
    "\tY_prediction = np.zeros((1, x_test.shape[1]))\n",
    "\n",
    "\t# if z is bigger than 0.5, our prediction is sign one (y_head = 1),\n",
    "\t# if z is smaller than 0.5, our prediction is sign zero (y_head = 0),\n",
    "\tfor i in range(z.shape[1]):\n",
    "\t\tif z[0, i]<= 0.5:\n",
    "\t\t\tY_prediction[0, i] = 0\n",
    "\t\telse:\n",
    "\t\t\tY_prediction[0, i] = 1\n",
    "\n",
    "\treturn Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d4f6569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration  0:  nan\n",
      "Cost after iteration  10:  nan\n",
      "Cost after iteration  20:  nan\n",
      "Cost after iteration  30:  nan\n",
      "Cost after iteration  40:  nan\n",
      "Cost after iteration  50:  nan\n",
      "Cost after iteration  60:  nan\n",
      "Cost after iteration  70:  nan\n",
      "Cost after iteration  80:  nan\n",
      "Cost after iteration  90:  nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 37.267080745341616 %\n",
      "test accuracy: 37.2093023255814 %\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, \n",
    "\t\t\t\t\t\tlearning_rate, num_iterations):\n",
    "\n",
    "\tdimension = x_train.shape[0]\n",
    "\tw, b = initialize_weights_and_bias(dimension)\n",
    "\t\n",
    "\tparameters, gradients, cost_list = update(\n",
    "\t\tw, b, x_train, y_train, learning_rate, num_iterations)\n",
    "\t\n",
    "\ty_prediction_test = predict(\n",
    "\t\tparameters[\"weight\"], parameters[\"bias\"], x_test)\n",
    "\ty_prediction_train = predict(\n",
    "\t\tparameters[\"weight\"], parameters[\"bias\"], x_train)\n",
    "\n",
    "\t# train / test Errors\n",
    "\tprint(\"train accuracy: {} %\".format(\n",
    "\t\t100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "\tprint(\"test accuracy: {} %\".format(\n",
    "\t\t100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "\t\n",
    "logistic_regression(x_train, y_train, x_test, \n",
    "\t\t\t\t\ty_test, learning_rate = 1, num_iterations = 100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15bee1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n\u001b[0;32m      2\u001b[0m logreg \u001b[38;5;241m=\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mLogisticRegression(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m, max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m----> 4\u001b[0m \tlogreg\u001b[38;5;241m.\u001b[39mfit(x_train\u001b[38;5;241m.\u001b[39mT, y_train\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mscore(x_test\u001b[38;5;241m.\u001b[39mT, y_test\u001b[38;5;241m.\u001b[39mT)))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m      6\u001b[0m \tlogreg\u001b[38;5;241m.\u001b[39mfit(x_train\u001b[38;5;241m.\u001b[39mT, y_train\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mscore(x_train\u001b[38;5;241m.\u001b[39mT, y_train\u001b[38;5;241m.\u001b[39mT)))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(random_state = 42, max_iter = 150)\n",
    "print(\"test accuracy: {} \".format(\n",
    "\tlogreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\n",
    "print(\"train accuracy: {} \".format(\n",
    "\tlogreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055d1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
